---
title: "Matching"
format:
  revealjs:
    theme: [clean, moon]
    width: 1280
    height: 720
    df-print: tibble
    scrollable: true
    smaller: true
    slide-number: true
    header: 
    self-contained: true
    mermaid:
      theme: forest
code-annotations: select
slide-level: 3
execute:
  echo: true
---

```{r setup, include=FALSE }
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)

```

```{css, echo = FALSE}
.output {
max-height: 500px;
overflow-y: scroll;
}
```

```{=html}
<script src = "https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js""></script>
<script type="text/javascript">
  $(document).ready(function() {
    $('body').prepend('<div class=\"zoomDiv\"><img src=\"\" class=\"zoomImg\"></div>');
    // onClick function for all plots (img's)
    $('img:not(.zoomImg)').click(function() {
      $('.zoomImg').attr('src', $(this).attr('src')).css({width: '100%'});
      $('.zoomDiv').css({opacity: '1', width: 'auto', border: '1px solid white', borderRadius: '5px', position: 'fixed', top: '50%', left: '50%', marginRight: '-50%', transform: 'translate(-50%, -50%)', boxShadow: '0px 0px 50px #888888', zIndex: '50', overflow: 'auto', maxHeight: '100%'});
    });
    // onClick function for zoomImg
    $('img.zoomImg').click(function() {
      $('.zoomDiv').css({opacity: '0', width: '0%'}); 
    });
  });
</script>
```

# Matching and subclassification

The experimental ideal allows us to assume this:

$$Y_i(0), Y_i(1), X_i \mathrel{\unicode{x2AEB}} D_i$$ But we could theoretically get the average treatment effect if we can assume that the assignment is **conditionally** independent of expected outcomes.

$$Y_i(0), Y_i(1) \mathrel{\unicode{x2AEB}} D_i |  X_i$$

Regression controls attempt to meet this more relaxed condition, but methods like matching and subclassification may be able to do so more flexibly.

# Subclassification

| Smoking group | Canada |  UK  |  US  |
|---------------|:------:|:----:|:----:|
| Non-smokers   |  20.2  | 11.3 | 13.5 |
| Cigarettes    |  20.5  | 14.1 | 13.5 |
| Cigars/pipes  |  35.5  | 20.7 | 17.4 |

: Death rates per 1,000 person-years. From: [Causal Inference Mixtape Ch. 5](https://mixtape.scunning.com/05-matching_and_subclassification)

## Subclassification

| Smoking group | Canada | British |  US  |
|---------------|:------:|:-------:|:----:|
| Non-smokers   |  54.9  |  49.1   | 57.0 |
| Cigarettes    |  50.5  |  49.8   | 53.2 |
| Cigars/pipes  |  65.9  |  55.7   | 59.7 |

: Mean ages. From: [Causal Inference Mixtape Ch. 5](https://mixtape.scunning.com/05-matching_and_subclassification)

## Subclassification

| Age group | Death rates | \# of Cigarette smokers | \# of Pipe or cigar smokers |
|:-------------:|---------------|:------------------:|:---------------------:|
| Age 20–40 | 20          |           65            |             10              |
| Age 41–70 | 40          |           25            |             25              |
| Age \>71  | 60          |           10            |             65              |
|   Total   |             |           100           |             100             |

: Death rates by age and smoking. From From: [Causal Inference Mixtape Ch. 5](https://mixtape.scunning.com/05-matching_and_subclassification)

## Subclassification

:::::: columns
::: {.column width="50%"}
| Age group | Death rates | \# of Cigarette smokers | \# of Pipe or cigar smokers |
|:-------------:|---------------|:------------------:|:---------------------:|
| Age 20–40 | 20          |           65            |             10              |
| Age 41–70 | 40          |           25            |             25              |
| Age \>71  | 60          |           10            |             65              |
|   Total   |             |           100           |             100             |

: Death rates by age and smoking. From From: [Causal Inference Mixtape Ch. 5](https://mixtape.scunning.com/05-matching_and_subclassification)
:::

:::: {.column width="50%"}
The un-adjusted death rate for cigarette smokers is:

$$
20 \times \frac{65}{100} + 40 \times \frac{25}{100} + 60 \times \frac{10}{100} = 29
$$

::: fragment
Their age-adjusted death rate is:

$$
20 \times \frac{10}{100} + 40 \times \frac{25}{100} + 60 \times \frac{65}{100} = 51
$$
:::
::::
::::::

## The area of common support

-   Subclassification is one way to adjust for differences using weighting, but there are some limitations here: what happens if I try to adjust for additional confounders? Or I try to group subjects into smaller age ranges?

| Age and Gender | Death rates | \# of Cigarette smokers | \# of Pipe or cigar smokers |
|:--------------:|----------------|:-----------------:|:--------------------:|
| Age 20 M | 0 | 10 | \- |
| Age 20 F | 1 | \- | 1 |
| Age 21 M | 3 | 1 | \- |
| Age 21 F | 0 | 7 | 2 |
| Age 22 M | 0 | \- | 2 |
| Age 22 F | 1 | 9 | 1 |
| ... | ... | ... | ... |

. . .

We don't have any way to "re-weight" the death rates at some values because there's no data in one side or another.

Methods like subclassification only work when there's sufficient overlap between treated and control units. This is sometimes referred to as the "area of common support" assumption.

## The area of common support and ATT

-   One way to mitigate the common support problem is to estimate the ATT instead of the ATE. If the missing data only appear in the treatment category, then we can still re-weight all of the non-zero treated units and recieve a valid ATT estimate:

| Age and Gender | Death rates | \# of Cigarette smokers | \# of Pipe or cigar smokers |
|:--------------:|----------------|:-----------------:|:--------------------:|
| Age 20 M | 0 | 10 | 1 |
| Age 20 F | 1 | \- | 1 |
| Age 21 M | 3 | 1 | 1 |
| Age 21 F | 0 | 7 | 2 |
| Age 22 M | 0 | \- | 2 |
| Age 22 F | 1 | 9 | 1 |
| ... | ... | ... | ... |

. . .

Note that, in some conditions, the ATT is a more sensible estimand anyways: everyone won't necessarily take the "treatment"!

# Matching

Matching is an related method for addressing confounding. In the simplest matching models, we would try to find identify control units with the similar values of all confounders for every treated unit, then compare results.

. . .

$$
\text{ATT} = \frac{1}{N_T}\sum_{D_i=1} (Y_i - Y_j(i))
$$ (Where $Y_j(i)$ is the matching unit for $Y_i$)

If the conditional independence assumption holds, then the average difference between the treated unit and its matched control should be equal to the difference in potential outcomes for the treated units.

## Matching

In cases where we have more than one match for a single treatment unit, we can average over the matches:

$$
\text{ATT} = \frac{1}{N_T}\sum_{D_i=1} (Y_i -\left[\frac{1}{M} \sum^M_{m=1} Y_{j_m(1)} \right]
$$ ... Where $M$ represents the number of matching units for $Y_i$

## Matching

Moreover, we *can* still estimate the ATE with this approach by finding counterfactuals for all treated and control units.

$$
{ATE} = \dfrac{1}{N} \sum_{i=1}^N (2D_i - 1) \bigg [ Y_i - \bigg ( \dfrac{1}{M} \sum_{m=1}^M Y_{j_m(i)} \bigg ) \bigg ]
$$

## Matching

...So how do we identify matching units? In most studies, "exact" matching is ideal, but unfeasible because of that common support requirement (especially when we want to adjust for continuous confounders like age)

. . .

But we can use various methods to find *similar* units instead of exact matches.

## Approximate Matching

For instance, we might use the Euclidean distance to compare vectors of covariates, then find the nearest match for each "point" (this is called nearest neighbor matching)

```{r}
library(ggplot2)

# Set seed for reproducibility
set.seed(123)

# Create 10 data points
df <- data.frame(
  x = runif(10, 0, 10),
  y = runif(10, 0, 10),
  color = rep(c("red", "black"), each = 5)
)

# Function to find nearest black point for each red point
find_nearest <- function(df) {
  red_points <- df[df$color == "red", ]
  black_points <- df[df$color == "black", ]
  
  arrows <- data.frame()
  
  for (i in 1:nrow(red_points)) {
    # Calculate distances to all black points
    distances <- sqrt((red_points$x[i] - black_points$x)^2 + 
                     (red_points$y[i] - black_points$y)^2)
    
    # Find nearest black point
    nearest_idx <- which.min(distances)
    
    # Create arrow data
    arrows <- rbind(arrows, data.frame(
      x = red_points$x[i],
      y = red_points$y[i],
      xend = black_points$x[nearest_idx],
      yend = black_points$y[nearest_idx]
    ))
  }
  
  return(arrows)
}

# Get arrow coordinates
arrows_df <- find_nearest(df)

# Adjust arrow endpoints to stop before the points
point_radius <- 0.15  # Adjust this to match your point size
arrows_df_adjusted <- arrows_df
for (i in 1:nrow(arrows_df_adjusted)) {
  # Calculate direction vector
  dx <- arrows_df$xend[i] - arrows_df$x[i]
  dy <- arrows_df$yend[i] - arrows_df$y[i]
  distance <- sqrt(dx^2 + dy^2)
  
  # Shorten from both ends
  arrows_df_adjusted$x[i] <- arrows_df$x[i] + (dx/distance) * point_radius
  arrows_df_adjusted$xend[i] <- arrows_df$xend[i] - (dx/distance) * point_radius
  arrows_df_adjusted$y[i] <- arrows_df$y[i] + (dy/distance) * point_radius
  arrows_df_adjusted$yend[i] <- arrows_df$yend[i] - (dy/distance) * point_radius
}

# Create plot
ggplot(df, aes(x = x, y = y)) +
  geom_segment(data = arrows_df_adjusted, 
               aes(x = x, y = y, xend = xend, yend = yend),
               arrow = arrow(length = unit(0.3, "cm")),
               color = "gray50",
               linewidth = 0.5) +
  geom_point(aes(color = color), size = 4) +
  scale_color_identity() +
  theme_minimal() +
  labs(title = "Nearest neighbor matching using Euclidean distance",
       x = "X1",
       y = "X2") +
  theme(plot.title = element_text(hjust = 0.5))

```

## Approximate Matching

(Many nearest-neighbor matching studies will use the Mahalanobis distance instead, which is a metric that accounts for correlations between covariates)

```{r}


# Set seed for reproducibility
set.seed(123)

# Create 10 data points
df <- data.frame(
  x = runif(10, 0, 10),
  y = runif(10, 0, 10),
  color = rep(c("red", "black"), each = 5)
)

# Function to find nearest black point for each red point using Mahalanobis distance
find_nearest <- function(df) {
  red_points <- df[df$color == "red", ]
  black_points <- df[df$color == "black", ]
  
  # Calculate covariance matrix from all points
  all_coords <- df[, c("x", "y")]
  cov_matrix <- cov(all_coords)
  
  arrows <- data.frame()
  
  for (i in 1:nrow(red_points)) {
    # Calculate Mahalanobis distances to all black points
    red_coord <- as.numeric(red_points[i, c("x", "y")])
    
    distances <- apply(black_points[, c("x", "y")], 1, function(black_coord) {
      diff <- red_coord - black_coord
      sqrt(t(diff) %*% solve(cov_matrix) %*% diff)
    })
    
    # Find nearest black point
    nearest_idx <- which.min(distances)
    
    # Create arrow data
    arrows <- rbind(arrows, data.frame(
      x = red_points$x[i],
      y = red_points$y[i],
      xend = black_points$x[nearest_idx],
      yend = black_points$y[nearest_idx]
    ))
  }
  
  return(arrows)
}

# Get arrow coordinates
arrows_df <- find_nearest(df)

# Adjust arrow endpoints to stop before the points
point_radius <- 0.15  # Adjust this to match your point size
arrows_df_adjusted <- arrows_df
for (i in 1:nrow(arrows_df_adjusted)) {
  # Calculate direction vector
  dx <- arrows_df$xend[i] - arrows_df$x[i]
  dy <- arrows_df$yend[i] - arrows_df$y[i]
  distance <- sqrt(dx^2 + dy^2)
  
  # Shorten from both ends
  arrows_df_adjusted$x[i] <- arrows_df$x[i] + (dx/distance) * point_radius
  arrows_df_adjusted$xend[i] <- arrows_df$xend[i] - (dx/distance) * point_radius
  arrows_df_adjusted$y[i] <- arrows_df$y[i] + (dy/distance) * point_radius
  arrows_df_adjusted$yend[i] <- arrows_df$yend[i] - (dy/distance) * point_radius
}

# Create plot
ggplot(df, aes(x = x, y = y)) +
  geom_segment(data = arrows_df_adjusted, 
               aes(x = x, y = y, xend = xend, yend = yend),
               arrow = arrow(length = unit(0.3, "cm")),
               color = "gray50",
               linewidth = 0.5) +
  geom_point(aes(color = color), size = 4) +
  scale_color_identity() +
  theme_minimal() +
  labs(title = "Nearest neighbor matching using Mahalanobis distance",
       x = "X1",
       y = "X2") +
  theme(plot.title = element_text(hjust = 0.5))

```

## Approximate Matching

The variations here are practically endless:

- For nearest neighbor matching, we might increase or decrease the number of matches permitted for each unit.

- We might also limit the distance between matched units so that, if nothing is close enough, we discard that match entirely.

- We might pair an approximate matching method with additional weighting methods to account for dissimilarities between pairs.

## Propensity scores

Instead of matching on values themselves, propensity score methods either re-weight or match observations based on a predicted propensity to receive treatment:

-   Estimate a model to predict **being in the treatment group**
-   Then either: 
     - Weight cases based on their inverse probability of treatment.
     - Match based on similar propensities (although this is now considered [bad practice](https://gking.harvard.edu/publications/why-propensity-scores-should-not-be-used-formatching))


## Propensity scores: inverse probability weighting

![From: https://www.andrewheiss.com/blog/2024/03/21/demystifying-ate-att-atu/](images/fig-propensity-treated-untreated-1.png)

## Propensity scores: inverse probability of treatment

![From: https://www.andrewheiss.com/blog/2024/03/21/demystifying-ate-att-atu/](images/fig-propensity-weighted-ate-1.png)




## Propensity scores

In practice propensity scores are often completely fine, but theoretically they can actually worsen bias if the treatment selection process isn't modeled correctly. Other methods are more robust to this kind of mis-specification.


## Coarsened Exact Matching

Coarsened exact matching works by:

::::: {.columns}
::: {.column width="50%"}
::: incremental
-   "Coarsening" the variables, prune unmatched observations (usually requires some user-input)
-   Conduct exact matching on the coarsened results
-   Reweight as needed to match groups
:::

:::
::: {.column width="50%"}


![](https://miro.medium.com/v2/resize:fit:1400/1*6p01hxNrmMiJ25-o_AeNoQ.png)

:::
:::::





```{r, include=FALSE}

library(ggdist)
library(modelsummary)
library(tidyverse)
library(ggpubr)
library(MatchIt)
library(cobalt)
library(ggdist)
library(broom)
data(lalonde.exp, package='causalsens')

lp<-lalonde|>
  mutate(group= factor(treat, labels=c('control', 'treated')))|>
  select(group, age, educ, re74, re75)

plt_list<-list()

for(i in c("age", "educ", "re74", "re75")){
  dens<-gghistogram(lp, x = i,
                 add = "mean", rug = TRUE,
                 color = "group", fill = "group",
                 add.params = list(linetype = "dashed",size=.7),
                 add_density = T,
              
                 palette = c("#00AFBB", "#E7B800")) 
  
  
  dens<-facet(dens, facet.by='group', ncol=1)
  
  stable <- desc_statby(lp, measure.var = i,
                        grps = "group")
  stable <- stable[,c('group', 'min', 'max', 'median', 'mean', 'iqr', 'sd', 'range')]
  stable.p <- ggtexttable(stable, rows = NULL,
                          theme = ttheme("classic"))
  
  plt_list[[i]]<-ggarrange(dens, stable.p,
                      ncol = 1, nrow = 2,
                      heights = c(1, 0.5))
}




plt_list[[1]] + 
  stat_compare_means(method = "t.test")


```

# Lalonde (1986)

-   National Supported Work experiment: qualified applicants randomly assigned to either a treatment or control condition. Treatment group received a guaranteed job for 9-18 months with additional counseling and support

. . .

-   Very positive results!

```{r, echo=F}

ll_exp<-lm(re78 ~ treat , data=lalonde.exp)
huxtable::huxreg(ll_exp)

```

## Lalonde (1986)

```{r}

library(ggstatsplot)
plt <- 
  lalonde.exp|>
  mutate(group = factor(treat, labels=c('control','treated')))|>

  ggbetweenstats(
  data = _,
  x = group,
  y = re78
)

plt

```

## Lalonde (1986)

-   Lalonde swapped the control group for a sample from the general population, and demonstrated that a similarly constructed study with observational data would fail to find the correct result, even after controlling for observed characteristics

```{r}

model<-lm(re78 ~ treat ,data=lalonde)
huxtable::huxreg("vs experimental control"=ll_exp,"vs population"=model)



```

## Lalonde (1986)

(he tried a bunch of stuff, none of it worked)

![](images/lalonde_results.png)

## Lalonde (1986)

Candidates for job training were unlike the general population. They weren't even especially similar to people with similar income, education, age, race, etc.

. . .

The treatment group differed from the general population in their expected **potential income**. So a naive difference of means would be a combination of the impact of job training and the difference in potential outcomes with the general population.

$$\underbrace{E[Y_i(1) - Y_i(0)|D_i=1]}_\text{job training effect} +  \underbrace{E[Y_i(0)|D_i=1] - E[Y_i(0)|D_i=0]}_\text{differences between applicants and non-applicants}$$

## Observed Imbalance

```{r, fig.width=10}
library(gtsummary)

ll2<-lalonde.exp|>
  mutate(educ = education, 
         race = case_when(hispanic ==1 ~ "hispan",
                          black == 1 ~ "black",
                          .default = "white"
                          )
         )|>
  
  filter(treat==0)|>
  
  mutate(treat = 0)|>
  select(colnames(lalonde))|>
  bind_rows(lalonde|>mutate(treat = treat+1))|>
  mutate(group = factor(treat,levels=c(1,0,2), labels=c('population','control', 'treated')))
  

tab<-ll2|>
   rename(`income 1974`=re74, 
         `income 1975`= re75
         )|>
  select(-treat, -re78)|>
  tbl_summary(
    data=_,
    #include = c(age, grade, response),
    by = group, # split table by group
    missing = "no" # don't list missing data separately
  ) |>
  add_n() |> # add column with total number of non-missing observations
 # add_p() |> # test for a difference between groups
  modify_header(label = "**Variable**") |> # update the column header
  bold_labels()

  
tab|>
    as_kable_extra(booktabs = TRUE) |>
  # reduce font size to make table fit. 
  # you may also use the `latex_options = "scale_down"` argument here.
  kableExtra::kable_styling(font_size = 16)

```

## Imbalance

Control variables probably can't fix this.

-   large observed differences often imply large unobserved differences
-   even if all confounding is observed and controlled, regression controls assume a linear relationship between the confounders and the treatment effects.

## Imbalance

Applied for program $\rightarrow$ Earnings

Applied for program $\leftarrow$ Education $\rightarrow$ Earnings

```{dot}
//| echo: false
//| fig-height: 5
//| fig-width: 5
digraph mrdag {

  graph [rankdir=TB]
  node [shape=oval, height=0.3, width=0.3, style=dashed]
 // S[label="Skills"]
  node [shape=box, height=0.3, width=0.3, style=solid]
  E[label="Education"]
  D[label="Applied for\n program"]
  Y[label="Earnings"]
  { rank = same; D,Y }
  //S -> E
 // S -> Y
 // S -> D
  E -> D
  D -> Y
  E -> Y
}
```

## Imbalance

```{r, echo=TRUE}
#| output-location: column

dataFunction<-function(){
  N<-1000
  education<-rpois(N, 3) # education
  D<-rbinom(N, 1, arm::invlogit(education)) # applied to job program
  Y_1<- 2 + education^2 +  rnorm(100) # potential outcome for training
  Y_0<- 0 + education^2 +  rnorm(100) # potential outcome for no training
  ATE<-mean(Y_1-Y_0) # the true effect 
  Y<-ifelse(D==1, Y_1, Y_0) # the observed outcomes 
  df<-tibble("income" =Y, Y_0=Y_0, Y_1=Y_1,
             'treated' = factor(D, labels=c("Control", "Treated")), 
             education)
  return(df)
}

set.seed(9999)
data<-dataFunction()
ggplot(data, aes(x=income,y=treated)) + 
  stat_halfeye() +
  theme_bw() +
  xlab("Potential Outcome if Y(1)")



mean(data$Y_1 - data$Y_0)



```

## Imbalance

Notably, since the education confounder has a non-linear effect on income, we don't actually get a correct estimate even if we control for it:

```{r}
ate <- mean(data$Y_1 - data$Y_0)
ols<-lm(income ~ treated, data=data)
ols_control<-lm(income ~ treated + education, data=data)

modelsummary(list('no controls'=ols, 'ols + control' =ols_control), 
             gof_map = c("nobs"),
             note = sprintf("actual ATE = %s", round(ate,digits=3))

             )
  

```


## Exact matching

```{r, echo=FALSE}

df<-data

matched<-matchit(treated ~ education ,data=df, 
                 method='exact',
                 estimand ='ATE'
                 )

md<-tibble('education'= factor(matched$X$education),  
                      'treatment' = factor(matched$treat, labels=c("control", "treatment")),
                      'subclass' = matched$subclass, 
                      'weight' =matched$weights,
                      'income' = data$income,
                      'dropped' = matched$weights==0,
                     "Y1" = data$Y_1
                     )|>
  arrange(treatment, education)




ggplot(md, aes(x=education, 
                         y=income, 
                         size = weight,
                         shape = dropped,
                         fill = treatment,
                         ), alpha=.5) + 
  geom_jitter(width=.5, height=.9)+
  theme_bw() +
  scale_fill_brewer(palette='Dark2', guide='none') +
  facet_wrap(~treatment, ncol=1) +
  scale_shape_manual(values=c(21, 4))


```

## Exact matching

```{r}


combined<-bind_rows(md, md|>mutate(weight=1),  .id='group')|>
  mutate(group=factor(group, labels=c("weighted",'unweighted')))

ggplot(combined, aes(x = group, y = Y1, fill = treatment)) +
  introdataviz::geom_split_violin(alpha = .4, trim = FALSE, aes(weight=weight)) +
  geom_boxplot(width = .2, alpha = .6, fatten = NULL, show.legend = FALSE, aes(weight=weight)) +
  scale_fill_brewer(palette = "Dark2", name = "") +
  theme_minimal() +
  ylab("Potential Income Y(1)")
  


```

## Exact matching

Exact matching on education gets us close to the correct answer:

```{r, echo=T}

ate<-mean(data$Y_1 - data$Y_0)

matched<-matchit(treated ~ education ,data=data, 
                 method='exact',
                 estimand ='ATE'
                 )
mdat<-match.data(matched)

exact_match_fit <- lm(income ~ treated + education,
           data = mdat, weights = weights)

modelsummary(list('no controls'=ols, 
                  'ols + control' =ols_control, 
                  'exact matching' =exact_match_fit), 
             gof_map = c("nobs"),
             note = sprintf("actual ATE = %s", round(ate,digits=3))
             )

```

## Exact matching

In repeated simulations, the exact matching estimates are unbiased and have only slightly more variance than the actual average treatment effect:

::::: {.columns}
::: {.column width="50%"}
```{r, echo=TRUE}



set.seed(999)
fun<-function(){
  df <- dataFunction() # re-using the data generating function
  ATE <- mean(df$Y_1 - df$Y_0)
  model<-lm(income ~ education + treated, data=df)
  matched<-matchit(treated ~ education ,data=df, method='exact' ,estimand='ATE')

  mdat<-match.data(matched)

  fit1 <- lm(income ~ treated ,
           data = mdat, weights = weights)
  res<-tibble('regression estimate' = coef(model)[3], 
                  #'matched' = mean(result$difference), 
              "matched estimate"=coef(fit1)[2],
              "Average Treatment Effect" = ATE
              )
return(res)

}

results<-replicate(500, fun(), simplify = F)|>
  bind_rows()|>
  pivot_longer(cols=everything())|>
  mutate(value = unlist(value))


```
:::
::: {.column width="50%"}

```{r}
ggplot(results, aes(x=value, fill=name, y=name)) + 
  theme_bw() +
  stat_halfeye(alpha=.7)+
  scale_fill_manual(values=c("#00AFBB", "#E7B800", "#D55E00")) +
  ylab("group")


```

:::
:::::



## Propensity scores

```{r, echo=T}


matched<-matchit(treated ~ education ,data=df, 
                 method ='nearest',
                 replace=TRUE,
                 distance='glm')
mdat<-match.data(matched)
#nrow(mdat)
psm_fit <- lm(income ~ treated + education , data=mdat, weights = weights)

modelsummary(list('ols + control' =ols_control, 
                  "exact matching"=exact_match_fit,
                  "psm" = psm_fit
                  ), 
             gof_map = c("nobs")
             )



```

## Propensity scores

```{r}
plot(matched, type = "jitter", interactive = FALSE)


```





## Coarsened Exact Matching

```{r, echo=T}
matched<-matchit(treated ~ education ,data=df, method='cem' ,
                 distance = 'glm', estimand = "ATE")
mdat<-match.data(matched)
cem_fit <- lm(income ~ treated + education , data=mdat, weights = weights)

modelsummary(list('ols + control' =ols_control, 
                  "exact matching"=exact_match_fit,
                  "psm" = psm_fit,
                  "cem" = cem_fit
                  ), 
             gof_map = c("nobs"),
             note = sprintf("actual ATE = %s", round(ate,digits=3))

             )



```

## Standard errors

Methods like CEM will generally group variables together before weighting, so we would expect to see clustering within the strata, but we know how to deal with this!

```{r, cache=TRUE}

marginaleffects::avg_comparisons(cem_fit, 
                                 variables='treated', 
                                 vcov='simulation',
                                 wts='weights',
                                 cluster= ~subclass
                                 
                                 )
```

## Takeaways

-   Matching *can* address some of the limitations of regression models and simulate experimental conditions.

-   Main advantages over regression are:

    -   No assumption of a linear relationship between confounders
    -   More informally: allows researchers to separate out the treatment selection process from the treatment effect process.
