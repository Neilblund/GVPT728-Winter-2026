---
title: "Other GLMs"
format:
  revealjs:
    theme: [clean, simple]
    df-print: paged
    smaller: true
    slide-number: true
    header: 
    header-logo: images/informal_seal_transparent.webp
    self-contained: true
    incremental: true
code-annotations: select
slide-level: 3
---

```{css, echo = FALSE}
.output {
max-height: 500px;
overflow-y: scroll;
}
```

```{r setup, include=FALSE }
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
library(MASS)
library(distributional)
library(tidyverse)
library(ggdist)
library(countrycode)
library(modelsummary)
library(labelled)
library(marginaleffects)

```

```{r, echo=FALSE, cache=TRUE}


vdem<-vdemdata::vdem|>
  filter(year == 2018)|>
  dplyr::select(country_id, year, v2x_libdem, e_gdppc)|>
  mutate(e_gdppc = e_gdppc /100)
gtd_counts<-read_csv('../Additional Code and Data/gtd_counts.csv')|>
  complete(country_txt , nesting(iyear), fill=list(nkill=0, n=0, success=0))|>
  group_by(country_txt)|>
  fill(region_txt, .direction='downup')|>
  group_by(country_txt)|>
  mutate(lagged_attacks = lag(n),
         lagged_deaths = lag(nkill)
         )|>
  filter(iyear == 2018)|>
  mutate(country_vdem = countrycode(country_txt, origin='country.name', dest='vdem'))|>
  left_join(vdem, by= join_by(iyear == year,  country_vdem == country_id))|>
  ungroup()|>
  mutate(any_attacks = factor(lagged_attacks>0, labels=c("no", "yes")),
         MENA = factor(region_txt =="Middle East & North Africa", 
                       labels=
                         c("no","yes"))
         )

gtd_counts<-set_variable_labels(gtd_counts, 
                                'v2x_libdem' ='liberal democracy score',
                                'region_txt' = 'region',
                                'success' = 'number of successful attacks',
                                'lagged_attacks' = 'prior year attacks',
                                'lagged_deaths' = 'prior year deaths',
                                'e_gdppc' = 'GDP per capita ($100,000)',
                                'any_attacks' = 'any attacks in prior year',
                                "MENA" = "Middle East & North Africa"
                                
                            
                                )

map<-c('v2x_libdem' ='liberal democracy score',
'region_txt' = 'region',
'success' = 'number of successful attacks',
'lagged_attacks' = 'prior year attacks',
'lagged_deaths' = 'prior year deaths',
'e_gdppc' = 'gdp per capita',
'any_attacksyes' = 'any attacks in prior year',
"MENAyes" = "Middle East & North Africa"

)

gtd_counts<-gtd_counts|>drop_na()

```

# Other generalized linear models

- Generalized linear models offer a framework for applying something like linear regression in all sorts of different contexts.

- All GLMs will have the same basic setup: we're predicting y as a function of some predictors and coefficients that are transformed by a link function:
  - In OLS, the "link" function is the identity, so $y = X^T\beta$
  - In logit models, the link function is $y = \text{logit}(X^T\beta)$
  - In probit models, the link function is $y = \Phi(X^T\beta)$

. . .

We'll talk about a couple other common models that use this same framework.



# Counts

:::::: columns
:::: {.column width="50%"}
Count models can be used to model rates or outcomes that range from 0 to $\infty$. Some examples might include:

::: incremental
-   Number of battle deaths in a conflict

-   Number of terror attacks in a country

-   Number of protests in a year

-   Number of years between coups (although this may be classified as a survival model)
:::
::::

::: {.column width="50%"}
![](images/Count_von_Count_kneeling.png)
:::
::::::

## Poisson Distribution

The simplest way to model a count is with a Poisson distribution: $y = \text{Poisson}(e^{X^t\beta})$

::::: columns
::: {.column width="50%"}
-   A poisson distribution has a single parameter, $\lambda$ , which represents the average rate of occurrence over a fixed time period.





:::

::: {.column width="50%"}
![](images/Poisson_pmf.svg.png)
:::
:::::

## Poisson Distribution

The expected value of $Y|X$ is

$$
E(Y|X) = e ^{(b_0 + b_1X_1 +b_2X_2...)}
$$

```{r}

poisson_func <- function(x,beta0,beta1) {
  theta <- exp(beta0 + beta1*x)
}
curve(poisson_func(x,0,1),xlim=c(-1,5), lwd=2,ylab="poisson_func function", las=1)
curve(poisson_func(x,-2,1),xlim=c(-1,5), lwd=2, lty=2, col="red", add=TRUE)
curve(poisson_func(x,2,1),xlim=c(-1,5), lwd=2, lty=2, col="blue", add=TRUE)

legend(-1,150,c(expression(paste(beta[0]," = 0")),
                expression(paste(beta[0]," = -2")),
                expression(paste(beta[0]," = 2"))), 
       lwd=2, lty=1:3, col=c("black","red","blue"))

```

## Poisson Distribution

```{r}

poisson_func <- function(x,beta0,beta1) {
  theta <- exp(beta0 + beta1*x)
}
curve(poisson_func(x,3,-2),xlim=c(-1,5), lwd=2,ylab="poisson_func function", las=1)
curve(poisson_func(x,3,0),xlim=c(-1,5), lwd=2, lty=2, col="blue", add=TRUE)
curve(poisson_func(x,3,2),xlim=c(-1,5), lwd=2, lty=2, col="red", add=TRUE)

legend(3,150,c(expression(paste(beta[1]," = 0")),
               expression(paste(beta[1]," = -2")),
               expression(paste(beta[1]," = 2"))), 
       lwd=2, lty=1:3, col=c("blue","black", "red"))

```


## Poisson model

As with logit and probit models, the link function means that our coefficient values are somewhat unintuitive. Technically, the represent the effect of a one unit change in X on the log count of the dependent variable.

```{r, echo=TRUE}
#| output-location: column

model<-glm(nkill ~e_gdppc ,
           data=gtd_counts, family='poisson')
modelsummary(list("Number killed" = model), 
             coef_rename=T, 
             estimate  = "{estimate}",  
             statistic = c("conf.int"),
             conf_level = .95,     
             note = "Note: 95% CI in brackets",
             gof_omit = 'F|RMSE|R2$',
             title ='Number killed in terrorist attacks in 2018'
             )

```


## Poisson model


Exponentiating the coefficients turns this into a multiplicative effect. So the coefficients represent the change in the *rate* of the dependent variable. i.e. a coefficient of 0.5 means that a one-unit increase in X halves the number of attacks, while a coefficient of 2 would mean doubling the number.

```{r, echo=TRUE}
#| output-location: column

model<-glm(nkill ~ e_gdppc,
           data=gtd_counts, family='poisson')
modelsummary(list("Number killed" = model), 
             coef_rename=T, 
             estimate  = "{estimate}",  
             statistic = c("conf.int"),
             conf_level = .95,     
             note = "Note: 95% CI in brackets",
             gof_omit = 'F|RMSE|R2$',
             title ='Number killed in terrorist attacks in 2018',
             exponentiate =TRUE
             )

```



## Poisson model predictions

For predicted counts, we can simply take the linear prediction from the model and then exponentiate it:

```{r}


x<-cbind(1, e_gdppc = seq(0, 2, by=.1))
predictions<-  exp(x%*% cbind(coef(model)))
data.frame(x, predictions)|>
  ggplot(aes(x=e_gdppc*100000, y=predictions)) + 
  geom_point() +
  geom_line()+
  theme_bw() +
  labs(x ='GDP Per Capita ($USD)', y='Predicted deaths') +
  scale_x_continuous(labels = scales::label_currency()) 


```


## Poisson Overdispersion

The poisson distribution assumes that the variance is equal to the mean (note that the distribution only has a single parameter: $\lambda$). This is rarely true for real-world count data:

```{r, echo=TRUE}

  # count of terrorist attacks per country-year
mean(gtd_counts$nkill)
var(gtd_counts$nkill)
data.frame(dist = dist_poisson(lambda =mean(gtd_counts$nkill)))|>
  ggplot(aes(xdist = dist))+
  stat_slab(alpha=.7) +
  stat_slab(data=gtd_counts, aes(x=nkill), 
               fill='lightblue', 
               inherit.aes = FALSE, alpha=.5) +
  theme_bw() +
  labs(title = 'Terrorist attacks in 2018 compared to a poisson distribution with the same mean') 

```

## Poisson Overdispersion

-   Models with more variance than expected are called "overdispersed", and this causes us to underestimate our standard errors.

    -   This can also happen with other distributions, but its probably less common because most distributions allow the variance to be different from the mean.

-   This is may be the result of different data generating processes, i.e.: one pattern for states with terrorist insurgencies, a separate pattern for those without.

    -   In some cases there may be a way to fix this with a different set of parameters, but there's no guarantee.
    


## Options for handling overdispersion

::: incremental
-   Quasipoisson models optimize a different likelihood function that can account for overdispersion.

-   A random effects model with one random effect per observation

-   Hurdle or zero-inflated models (in special cases)

-   **A negative binomial model**
:::

### Negative Binomial Model

-   The negative binomial distribution can be thought of as a way of modeling the number of failures until a specified number of successes. Unlike the poisson distribution, the negative binomial distribution has a mean and a variance, so it can account for overdispersion:

```{r, echo=TRUE}

tibble('a' = dist_negative_binomial(size=1, .5),
       'b' = dist_negative_binomial(size=10, .5),
       'c' = dist_negative_binomial(size=100, .5)
       )|>
  ggplot()+
  stat_slab(aes(xdist = a), fill='lightblue', alpha=.7) +
  stat_slab(aes(xdist = b), fill='lightgreen', alpha=.7) +
  stat_slab(aes(xdist = c), fill='orange', alpha=.7) +
  theme_bw() 

```

### Poisson vs. negative binomial



```{r, echo=TRUE}
model<-glm(nkill ~ e_gdppc ,
           data=gtd_counts, family='poisson')
nb_model<-glm.nb(nkill ~ e_gdppc , 
                 data=gtd_counts)

```



```{r, echo=FALSE}


map<-c('v2x_libdem' ='liberal democracy score',
'region_txt' = 'region',
'success' = 'number of successful attacks',
'lagged_attacks' = 'prior year attacks',
'lagged_deaths' = 'prior year deaths',
'e_gdppc' = 'gdp per capita',
'any_attacksyes' = 'any attacks in prior year',
"MENAyes" = "Middle East & North Africa"

)

model_list<-list('poisson' = model,
                 "negative binomial" = nb_model)
modelsummary(model_list,
             coef_rename = TRUE,
             estimate  = "{estimate}",  
             statistic = c("conf.int"),
             conf_level = .95,     
             note = "Note: 95% CI in brackets",
             gof_omit = 'F|RMSE|R2$',
             title ='Number killed in terrorist attacks in 2018',
             exponentiate =FALSE
             )

```

### Poisson vs. negative binomial

The inverse link for the negative binomial and poisson models are the same, so the exponentiated coefficients have the same interpretation as they did for the poisson model:


```{r}
#| output-location: column


model_list<-list('poisson' = model,
                 "negative binomial" = nb_model)
modelsummary(model_list,
             coef_rename = TRUE,
             estimate  = "{estimate}",  
             statistic = c("conf.int"),
             conf_level = .95,     
             note = "Note: 95% CI in brackets",
             gof_omit = 'F|RMSE|R2$',
             title ='Number killed in terrorist attacks in 2018. Exponentiated coefficients.',
             exponentiate =TRUE
             )

```





## Testing Overdispersion

The performance package has a function to check for overdispersion using simulated residuals from a count model. The null hypothesis here is no overdispersion:

```{r}
library(performance)
check_overdispersion(model)


```

We can also see this is addressed by the negative binomial model:

```{r}


check_overdispersion(nb_model)

```

## Model fit

Since the only difference here is the addition of a single dispersion parameter, we can treat these two models as nested and use a likelihood ratio test to determine with the negative binomial model is a better fit. Unsurprisingly, it is:

```{r}

pchisq(2 * (logLik(nb_model) - logLik(model)), df = 1, lower.tail = FALSE)


```



## Marginal Effects



```{r}

mfx<-avg_comparisons(nb_model)

modelsummary(mfx, 
             coef_rename = TRUE,
             estimate  = "{estimate}",  
             statistic = c("conf.int"),
             conf_level = .95,     
             note = "Note: 95% CI in brackets",
             gof_omit = 'F|RMSE|R2$',
             title ='Number killed per attack. Exponentiated coefficients.',
             )
  

```

## Using a Random Effect

A random effects model also provides and alternative method to account for overdispersion by adding a single random effect term for every case in the model:

```{r}

library(lme4)


mmodel<-glmer( nkill ~ 
                e_gdppc +
                (1|country_txt), data=gtd_counts, family='poisson')



performance::check_overdispersion(mmodel)




```

## Count models with an offset

Poisson and negative binomial models can also be used to model ratio. For instance, we might want to model the number of deaths per attack rather than just the number of deaths in general. To do this, we'll need to include the logged number of attacks as an "offset" variable in the model. We'll also have to remove cases with zero attacks, since it doesn't make sense to measure a rate where the denominator is zero:

```{r}
#| output-location: column

deaths_per_attack<-gtd_counts|>
  filter(n > 0)|>
  glm.nb(nkill ~ e_gdppc  + offset(log(n)),
           data=_)


modelsummary(deaths_per_attack, 
             coef_rename = TRUE,
             estimate  = "{estimate}",  
             statistic = c("conf.int"),
             conf_level = .95,     
             note = "Note: 95% CI in brackets",
             gof_omit = 'F|RMSE|R2$',
             title ='Number killed per attack. Exponentiated coefficients.',
             exponentiate =TRUE
             )



```



# Considerations

-   For count data, a poisson model is a starting point, but over-dispersion is so common in real-world data that you should never assume it and you should be a bit skeptical if you a poisson with no discussion of the problem





# Multiple categories

- Logit/probit models work for a single dichotomous outcome, but how do we model cases with more than two possibilities?


## Ordered logits

Ordered logit/probit models can be used to model the effect of a one unit increase in some **ordered** categorical outcome, such as an agree/disagree scale.

One way to think about these models is that they work like the latent data-formulation of the logit model, but there are multiple thresholds that may be unequally spaced:



```{r, echo=FALSE}
set.seed(1000)
N<-500
x<-rnorm(N)
ystar <- x * 3

cutpoints<-c(-5,  0, 9)
ycut<-cut(ystar + rlogis(N), c(-Inf,-9, -5, 0,  6, Inf),
          labels=c("lowest","lower", "medium", "higher", "highest")
          ) 

df<-data.frame(x, ystar, ycut)


ggplot(df, aes(x=x, y=ystar, color=ycut)) + 
  geom_jitter(alpha=.5, height=1) +
  theme_minimal()+
    geom_hline(yintercept = -8, lty=2, color='black') +
  geom_hline(yintercept = -5, lty=2, color='black') +
  geom_hline(yintercept = 0, lty=2, color='black') +
  geom_hline(yintercept = 6, lty=2, color='black') +
  labs(color ='category', y='Y*', x='X')





```
. . .


Our goal here would be to estimate both the effect of X on the latent outcome Y* and the thresholds for each of the response categories.

## Ordered logits

In a sense, you could think of this as a series of K-1 logistic regressions for each level of the ordered category:


$$
\begin{align*}
Pr(Y > 1) & = \text{logit}^-1(X\beta) \\
Pr(Y > 2) & = \text{logit}^-1(X\beta- \text{cut}_2) \\
Pr(Y > 3) & = \text{logit}^-1(X\beta -\text{cut}_3) \\
...\\
Pr(Y > K-1) & = \text{logit}^-1(X\beta - \text{cut}_{K-1}) \\
\end{align*}
$$


## Ordered logits


```{r}
set.seed(1000)
N<-1000
x<-rnorm(N)
ystar <- x * 3

cutpoints<-c(-5,  0, 9)
ycut<-cut(ystar + rlogis(N), c(-Inf,-9, -5, 0,  6, Inf),
          labels=c("lowest","lower", "medium", "higher", "highest")
          ) 

df<-data.frame(x, ystar, ycut)
MASS::polr(ycut ~ x, data=df, method='logistic')

```
. . .

The terms labelled as `intercepts` here correspond to the cutpoints in my simulated latent variable, and the coefficient on X is just the effect of `x` on `ystar`


## Ordered logits

To get predictions for individual models, we'll need to use the inverse of the logit function and calculate 

$$
Pr(y = k) = \text{logit}^{-1}(X\beta-c_{K-1}) - \text{logit}^{-1}(X\beta-c_k)
$$

## Ordered logits: predictions

We'll get the coefficient values and the cutpoints from the model:

```{r}
invlogit<-function(x){
  return(exp(x)/(1+exp(x))) 
  
}
ologit<-MASS::polr(ycut ~ x, data=df, method='logistic')
# coefficients
coefs<-ologit$coefficients
# intercepts
cutpoints<-ologit$zeta
```

## Ordered logits: predictions


Now we'll calculate individual response probabilities:

```{r}

# Pr(k = 1) if X =2
invlogit(cutpoints[1] - 2 * coefs)
# Pr(k = 2) if X =2
invlogit(cutpoints[2] - 2 * coefs) - invlogit(cutpoints[1] - 2 * coefs)
# Pr(k = 3) if X =2
invlogit(cutpoints[3] - 2 * coefs) - invlogit(cutpoints[2] - 2 * coefs)
# Pr(k = 4) if X =2
invlogit(cutpoints[4] - 2 * coefs) - invlogit(cutpoints[3] - 2 * coefs)
# Pr(k = 5) if X =2
1 - invlogit(cutpoints[4] - 2 * coefs)

```


## Ordered logits

Of course, we can also just use `predict` to get these same results:

```{r}

predict(ologit, type='probs', newdata=data.frame(x=3))

```

## Ordered logits

Or we can use the `ggeffects` package to get predictions across all values of `x` and all levels of the outcome:

```{r}
library(ggeffects)

result<-predict_response(ologit)

plot(result)

```

## Ordered logits

Here's a more practical example using European Social Survey data for Great Britain. The dependent variable is whether richer countries have an obligation to take immigrants, and the predictors are whether the respondent has been unemployed in the last 5 years, and their self placement on a left/right scale:

```{r, echo=FALSE}

gb<-readRDS("C:/Users/neilb/Documents/UMD/GVPT728-Winter-2026/Additional Code and Data//great_britain.rds")

```



```{r}


ologit<-MASS::polr(richer_responsible ~ uemp5yr + lrscale, data=gb, 
                   method='logistic')
ologit

```

## Ordered logits


```{r}
#| layout-ncol: 2
preds<-predict_response(ologit)
pp<-plot(preds)
pp$uemp5yr
pp$lrscale


```



## Ordered logits

And we can get estimated average marginal effects from the model as well. Note that we have 5 different margins for each coefficient, one for each category of the DV:

```{r, eval=FALSE}
library(marginaleffects)
mfx<-avg_comparisons(ologit)

mfx



```

```{r}
library(marginaleffects)
avg_comparisons(ologit)|>
  dplyr::select(term, group, contrast, estimate, conf.low, conf.high)|>
  mutate(across(where(is.numeric), .fns=~round(.x, digits=4)))

```

## Ordered logits: considerations

- Ordered logits make the most sense when you have a relatively small number of  ordered categories and you think the differences between the responses are "unevenly spaced" with respect to your IVs

- If you have a lot of response categories (7 or more) a linear model if generally a reasonable alternative.

- Even in cases with a smaller number of categories, the juice may not be worth the squeeze, but an ordered logit might be worth exploring if you're unsatisfied with your results.



## Other models



-   Multinomial logit: responses for multiple discrete categories
    -   K-1 coefficients for each category of the DV.
    -   Coefficients are the effect on the logged odds of being in a category relative to the baseline group.




