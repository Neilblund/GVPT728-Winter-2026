---
title: "Maximum likelihood, logit and probit and LPMs"
format:
  revealjs:
    theme: [clean, custom_styles]
    df-print: paged
    smaller: true
    slide-number: true
    header: 
    header-logo: images/informal_seal_transparent.webp
    self-contained: true
    incremental: true
code-annotations: select
slide-level: 3
---

```{r setup, include=FALSE }
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
library(tidyverse)
library(dataverse)
library(modelsummary)

```

```{css, echo=FALSE}
.table, th, td {
  font-size: 0.9em;
} 
.table tbody td, .table thead tr {
    white-space: nowrap;
}
```

```{r, include=FALSE, cache=TRUE}
library(tidyverse)


navco <- dataverse::get_dataframe_by_name(
  filename = 'NAVCO 1.2 Updated.tab',
  dataset = 'doi:10.7910/DVN/0UZOTX', 
  server = "dataverse.harvard.edu")


navco<-navco|>
  filter(ONGOING==FALSE)|>
  mutate(goal_type = case_when(
    REGCHANGE == 1 ~ "Regime Change",
    FSELFDET == 1 ~ "Self determination",
    SECESSION == 1 ~ "Secession",
    OTHER == 1 ~ "Other"
    
  ),
  outcome = factor(SUCCESS, labels=c('failure', 'success')),
  percent_pop  = `PERCENTAGEPOPULARPARTICIPATION`,
  participation_ntile = findInterval(
    `PERCENTAGEPOPULARPARTICIPATION`,
    
    quantile(`PERCENTAGEPOPULARPARTICIPATION`, seq(0,.9, by=.1)))
  )

navco$goal_type<- fct_relevel(factor(navco$goal_type), 'Self determination')






```

# Limited dependent variables

## The problem

-   Many important outcomes in the social sciences are non-continuous

    ::: incremental
    -   Binary or categorical outcomes: vote choice, choosing to ride a bike instead of driving, or winning an election

    -   Counts: number of trade agreements, number of gun deaths in a city, or number of doctor visits in a year

    -   Durations: length of a military conflict, the longevity of a coalition, or the time spent on unemployment
    :::

## The problem

Fitting a line to this kind of outcome often produces nonsensical results: predictions below the minimum or above the maximum and slopes that have impossible increments. Moreover, the functional form is likely incorrect even where the predictions are possible because the slope depends on the current expected value of the dependent variable.

```{r, echo=FALSE}
set.seed(1000)
N<-100
X<-rnorm(100)
Y_count<-rpois(N, exp(1 + X))
Y_binomial<-rbinom(N, 1, arm::invlogit(1+X))
Y_duration<-rexp(N, exp(X))

df<-data.frame(X, Y_count, Y_binomial, Y_duration)|>
  pivot_longer(cols = Y_count:Y_duration, values_to="Y")

ggplot(df, aes(x = X, y =Y, group=name)) + geom_point() + 
  geom_smooth(method='lm', se=FALSE) +
  facet_wrap(~name, scales='free') +
  theme_minimal()

```

## The problem

And the resulting standard errors will be non-normal and heteroskedastic by definition:

```{r, echo=FALSE}


mods<-df|>
  split(~name)|>
  map(\(x) lm(Y ~ X, data=x))|>
  map(broom::augment)|>
  bind_rows(.id='model')


ggplot(mods, aes(x=.fitted, y=.resid)) + 
  geom_point() + 
  facet_wrap(~model, scales='free') +
  geom_hline(yintercept=0, lty=2, col='red') +
  theme_minimal()



```

## NAVCO data

The [NAVCO](https://dataverse.harvard.edu/dataverse/navco) project collects data on "maximalist campaigns", which are defined as sustained efforts to remove a government, attain territorial independence, or make fundamental changes to an existing constitutional order. A key question motivating the project is "what sorts of tactics are effective at creating major political change?"

```{r}
navco|>select(LOCATION, CAMPAIGN, BYEAR, SUCCESS, VIOL, DURATION, REGAID, STATESUP, goal_type)|>head()


```

# Some options other than logit/probit

## Solutions: contingency table methods

-   For models with a **single categorical predictor and a limited number of outcome categories**, contingency table methods can give you a valid test of statistical significance

    -   The $\chi^2$ and Fisher's Exact Test will work for categorical by categorical relationships.

    -   Spearman's $\rho$ , Kendall's $\tau$, and Somer's $D$ can test ordered relationships.

### Contingency table methods

$P<.05$ in Fisher's Exact test means a less than 5% chance of getting these cell frequencies if the rows and columns are uncorrelated.

```{r, echo=T}
library(gtsummary)
navco|>

  select(outcome, goal_type)|>
  tbl_summary(by=goal_type)|>
  #add_p(test= outcome  ~'chisq.test') # for x^2 test

  add_p(test= outcome  ~'fisher.test')


```

### Considerations: contingency table methods

-   Methods like Fisher's Exact test are non-parametric and will be more conservative at smaller sample sizes compared to regression

-   Only valid for categorical-by-categorical relationships

## Solutions: linear probability model

### Linear Probability Models

::::::: columns
::::: {.column width="50%"}
:::: columns
Assuming the DV is coded as 0 or 1:

::: incremental
-   The constant is the proportion of successes for the omitted category

-   Each coefficient is the difference in the proportion of success compared to the constant.

-   Since these are all mutually exclusive categories, none of the predictions will be above 1 or below 0.
:::
::::
:::::

::: {.column width="50%"}
```{r}
navco_lpm<-lm(SUCCESS ~  goal_type, data=navco)


```

```{r, echo=FALSE}

coefs<-c('goal_typeRegime Change' = 'Goal: Regime change',
         'goal_typeSecession' = 'Goal: Secession',
         'goal_typeSelf determination' = 'Goal: Self-determination',
         'goal_typeOther' = 'Goal: Other',
         "(Intercept)"= "(Reference category) Goal: Self-determination"
         )

navco_lpm<-lm(SUCCESS ~  goal_type, data=navco)


modelsummary(list("LPM" =navco_lpm), 
              coef_rename=coefs,
              estimate  = "{estimate}",  
             statistic = c("conf.int"),
             conf_level = .95,        
 note = "95% CI in brackets",
 gof_omit = 'F|RMSE|R2$|AIC|Log.Lik.',
 output ='kableExtra',
             )
```
:::
:::::::

### Considerations: Linear Probability Model

-   Good: If there is a single binary predictor, then a linear probability model is probably fine.

    -   It may even be fine even for more complex models, especially if the primary goal is just identify the average effect of an IV and test for significance.

-   Violates Gauss Markov assumptions

-   Will generate nonsensical results (probabilities greater than 1 or less than zero)

## Solutions: machine learning methods

Many machine learning methods can handle all sorts of weird data structures and many of them perform better than logit/probit models when predicting binary outcomes.

However! Machine learning means optimizing for prediction rather than hypothesis testing.

. . .

(technically, logistic regression is itself a machine learning method. The distinction here is mostly a question of research goals)

### Classification trees

Tree-building algorithms use a process of recursively splitting the data at different levels of the predictors to produce decision tree. The model is non-parametric and can account for all sorts of complex non-linearity and interactive relationships in the predictors.

### Classification trees

Most algorithms will perform automatic variable selection as part of the inference process, and a variety of modification exist to reduce the tendency for these models to become overfitted.

(ideally in this case you would want to create a held-out set and compare the predictive quality of different models)

```{r, echo=TRUE}
library(rpart)
library(rpart.plot)
tree <- rpart(outcome ~ VIOL + STATESUP + REGAID + DEFECT  + participation_ntile, data = navco)
rpart.plot(tree)

```

### Considerations: Machine Learning classifiers

-   If you pick up a textbook on Machine Learning, you'll probably find some discussion of logistic regression, but the emphasis will generally be on prediction rather than hypothesis testing

-   Alternative classification methods like trees, support vector machines, bagging, neural networks etc. can often handle non-linearity, overfitting, interaction effects etc. more easily than models like logit/probit, but this flexibility can also make them "black boxes" that are harder to interpret.

# Logit and probit models

(The following sections will deal with models for binary outcomes, but the general intuition applies to other kinds of discrete choices)

## The Latent variable intuition

::::: columns
::: {.column width="50%"}
One way to think about a binary outcome is as the result of some continuous latent variable that depends on the predictors:

$$Y^*_i = \beta X_i + \epsilon_i $$ That generates a $1$ if $Y > 0$ and $0$ otherwise.

In a logit, we assume the cumulative distribution function of $\epsilon_i$ follows a Logistic distribution. In a probit, we assume it comes from a Gaussian. Understood in this sense, this is really the only difference.
:::

::: {.column width="50%" style="fragment"}
```{r, echo=FALSE}
library(ggdist)
library(tidyverse)
library(distributional)

tibble(
  lambda = c(-3, -2,-1, 0, 1,2,3),
  group  = factor(lambda)
) |>
  ggplot(aes(x = lambda)) +
  stat_slab(aes(ydist = dist_logistic(lambda, 1), fill = after_stat(y>0)), alpha=.5, color='black', lwd=.1) +
  geom_abline(slope=1, intercept = 0, lty=3) +
  geom_hline(yintercept=0, lty=1, col='black', lwd=.1) +
  labs(fill = "Y", 
       x= 'X',
       y= 'Y-star'
       ) +
  ggtitle("") +
  theme_bw() +
  scale_fill_manual(values=c("FALSE"='white', "TRUE" = "orange")) +
  theme(legend.position="none")



```
:::
:::::

## Logit and Probit with latent variables

::::: columns
::: {.column width="50%"}
```{r, echo=T}

set.seed(100)
N<-1000
X<-rnorm(N, 0, 1)
ystar<-X * 2 
# the probit link
y_probit<-(ystar + rnorm(N))>0
# the logit link 
y_logit <-(ystar + rlogis(N))>0
probit<-glm(y_probit ~ X, family=binomial(link = probit))
logit<-glm(y_logit ~ X, family=binomial(link = logit))

```
:::

::: {.column width="50%"}
```{r, echo=FALSE}

modelsummary(list("logit" =logit, 
                  'probit' = probit
                  ), 
              coef_rename=coefs,
             coef_omit = "Intercept",
              estimate  = "{estimate}",  
             statistic = c("conf.int"),
             conf_level = .95,        
 note = "95% CI in brackets",
 gof_omit = 'F|RMSE|R2$|AIC|Log.Lik.',
 output ='kableExtra'
             )
```
:::
:::::

## The Logistic Function

Changing the "intercept" term here will shift the curve to the left or the right

```{r, echo=FALSE}
logit <- function(x,beta0,beta1) {
  1/(1+exp(-beta0 - beta1*x))
}
curve(logit(x,0,1),xlim=c(-10,10), lwd=2,ylab="Logistic Function", las=1)
curve(logit(x,-2,1),xlim=c(-10,10), lwd=2, lty=2, col="red", add=TRUE)
curve(logit(x,2,1),xlim=c(-10,10), lwd=2, lty=2, col="blue", add=TRUE)
legend(-10,1,c(expression(paste(beta[0]," = 0")),expression(paste(beta[0]," = -2")),expression(paste(beta[0]," = 2"))), lwd=2, lty=1:3, col=c("black","red","blue"))



```

## The Logistic Function

Changing values of the slope will make the line steeper (especially around the middle)

```{r, echo=FALSE}
curve(logit(x,0,1),xlim=c(-10,10), lwd=2,ylab="Logistic Function", las=1)
curve(logit(x,0,2),xlim=c(-10,10), lwd=2, lty=2, col="red", add=TRUE)
curve(logit(x,0,0.5),xlim=c(-10,10), lwd=2, lty=3, col="blue", add=TRUE)
legend(-10,0.9,c(expression(paste(beta[1]," = 1")),expression(paste(beta[1]," = 2")),expression(paste(beta[1]," = 0.5"))), lwd=2, lty=1:3, col=c("black","red","blue"))

```

::: notes
The real difference in logit and probit models lies in their assumptions about that error term. Logistic assumes it follows a standard logit. Probit assumes it follows a standard normal. In practice, this should matter very little.
:::

## Maximum likelihood

-   OLS tries to minimize the sum of squared errors, but we can't actually infer that here because we can't see the latent values of y.

-   Instead, maximum likelihood methods attempt to identify the function that maximizes the likelihood of the observed data.

## Likelihood vs. Probability

Likelihood has a specific meaning in this context that's distinct from probability:

-   The likelihood is the probability density of the *data* at a given set of parameter values.

-   The likelihoods of different models won't necessarily sum to 1, so likelihoods are unbounded and only really meaningful as a measure of the relative fit of a model to some data.

## Likelihood vs. Probability

If I flip a coin 10 times and get 5 heads and 5 tails, what is the likelihood that this data came from a coin where the probability of heads is .25?

We can use the Bernoulli distribution to get the probability of a coin landing on heads for a single "trial":

$$
f(x) =  {n\choose x}  p^{x}(1-p)^{n-x}
$$

$p$ is the probability of heads, and $x$ is the number of heads, and n is the total number of flips.

$$
f(5|.25) = {10\choose 5} .25^5(1-.25)^{10-5} = .0.0583992
$$

## Maximizing the likelihood

Even without calculating the probabilities for all possible values of $p$, we can tell that .4 is more likely to have generated the data:

$$
f(5|.4) = {10\choose 5} .4^5(1-.4)^{10-5} = 0.2006581
$$ ... so our goal here is to find an estimate of p that is most likely to have generated our data by maximizing the function above.

## Maximizing the likelihood

We can derive an estimator that will identify the model that maximizes the probability of the observed data with some calculus.

Find the derivative of $L(p)$ with respect to $p$

$$
\begin{align*} L(p) &= { n \choose k } p^k (1-p)^{n-k} \\ \frac{d}{dp}L(p) &= { n \choose k } \big( kp^{k-1} (1-p)^{n-k} + p^k (n-k)(1-p)^{n-k-1}(-1) \big) \end{align*}
$$

. . .

Setting to zero

$$
\begin{align*}{n \choose k }\cdot \left( kp^{k-1} (1-p)^{n-k} + p^k (n-k)(1-p)^{n-k-1}(-1) \right) &= 0\end{align*}
$$

## Maximizing the likelihood

Solving...

$$
\begin{align*} { n \choose k }\cdot \left( kp^{k-1} (1-p)^{n-k} + p^k (n-k)(1-p)^{n-k-1}(-1) \right) &= 0 \\ kp^{k-1} (1-p)^{n-k} - p^k (n-k)(1-p)^{n-k-1} &= 0 \\ kp^{k-1} (1-p)^{n-k} &= p^k (n-k)(1-p)^{n-k-1} \\ k (1-p) &= p (n-k) \\ k - p k &= p n - p k \\ k &= p n \\ p^* &= \boxed{\frac{k}{n}} \end{align*}
$$

. . .

The maximum likelihood estimate is $\frac{k}{n}$, which is a very complex way of saying the sample proportion is a good estimate of the population proportion.

## Maximum likelihood

-   This method of maximizing the likelihood is useful because it gives us a lot more flexibility for finding a "best fitting" model, even when we're not dealing with straight lines (although we can also derive [a maximum likelihood estimator for the linear model](https://statproofbook.github.io/P/slr-mle.html)!

Two notes:

-   For computational reasons, we'll usually work with the log likelihood rather than the likelihood, but the basic logic is the same.

-   In most regression models, there's typically no "closed form" solution to finding the maximum likelihood estimate like we had for coin flips. So we'll typically approximate it using numerical methods.

## Maximum likelihood for a logistic regression

The likelihood function for a logistic regression is also derived from the Bernoulli distribution, and it looks like this:

$$
\ln L(\beta_0,\beta_1) = \sum_{i=1}^N \{ y_i \ln p(x_i; \beta_0,\beta_1) + (1-y_i) \ln [1-p(x_i;\beta_0,\beta_1)]
$$

. . .

(technically optimization algorithms will minimize the inverse of this function)

## Maximum Likelihood

```{r}
set.seed(400) 
N <-5000 
alpha <- 0
beta <- 1
d<-tibble(
  X = rnorm(N), # random IVs 
  y_star = alpha + X * beta,  # a latent variable
  Y = as.numeric((y_star + rlogis(N)) >0) # 1s and 0s (This is what we can observe!)
)

# the negative log likelihood function for the logistic model: 
neg_log_likelihood = function(beta, x, y){
  theta = beta * x
  p <- exp(theta) / (1 + exp(theta))
  val = -sum(y * log(p) + (1-y)*log(1-p)) 
  return(val)
}



```

## Maximum Likelihood

Note that the negative log likelihood is minimized at the the correct value of $\beta_1$:

```{r, echo=FALSE}

likelihoods<-tibble(
  beta = seq(-10, 10, by=.1), 
  neg_ll = sapply(beta, function(b) (expr = neg_log_likelihood(b, d$X, d$Y)))
  )

ggplot(likelihoods, aes(x=beta, y=neg_ll)) +
  geom_line() +
  geom_vline(xintercept=1, col='red', lty=2) +
  theme_bw() +
  ylab("Negative Log-Likelihood")

```

## Bayesian Inference

-   Maximum likelihood methods have a close relationship with Bayesian inference. One way to think about Bayesian inference is that it represents a kind of synthesis of likelihood derived from the data and a subjective *prior* that represents your pre-existing expectations about the model parameters.

-   In theory, Bayesian methods give us a way to take the probability of the data given a model ($p(D|\theta$) and convert it to something closer to "the probability of a model given the data and a prior" ($p(\theta | D)$)

. . .

$$p(\theta | D) = \frac{p(D|\theta) p(\theta)} {p(D)}$$

. . .

-   The compromise between the prior and the likelihood is called the **posterior** distribution.

## Bayesian Inference

When data are limited and variance is high, the prior will be really important for our estimates, when there's lots of data and low variance, the prior becomes less influential and the data becomes more important.

Similarly, the prior itself can be diffuse or narrow, or uniform, or centered around zero.

::::: columns
::: {.column width="50%"}
![](images/prior2post_2-1.svg)
:::

::: {.column width="50%"}
![](images/prior2post_3-1.svg)
:::
:::::

## Bayesian Inference vs. MLE

-   Priors are useful because they give us a way to incorporate existing information into our models: if prior research suggests $\beta_0$ should be .5, I can use that as a starting point and then update with new data.

-   They're also kind of problematic, because they introduce an element of subjectivity into inference: who can say if my priors are the correct ones?

-   Priors also can give more "stable" estimates of coefficients when we have small samples or lots of parameters, because the prior penalizes extreme estimates.

-   In many cases, Bayesian and MLE estimates are going to be very similar, but there are some scenarios where Bayesian methods have some theoretical or practical advantages over frequentist models.

-   Bayesian inference is almost always slower than MLE methods, because it requires a more complex set of calculations.

## Bayesian vs. MLE logit in R


```{r, eval=FALSE}

# MLE 
navco_logit<-glm(SUCCESS ~  goal_type, data=navco,  family = binomial(link = "logit"))

# Bayesian
navco_blogit<-stan_glm(SUCCESS ~  goal_type, data=navco,   
                       family = binomial(link = "logit"),
                       seed = 100,  # random number seed - because the inference is non-deterministic
                       prior = normal(0, 5), # normal prior with mean = 0 and sd = 5
                       refresh = 0 # turns off progress output, remove this to see how the model is progressing
                       )

modelsummary(list("MLE"= navco_logit, "Bayesian" = navco_blogit),
             statistic = c("conf.int"),
             gof_map = c('nobs','logLik'),
             coef_rename =coefs,
             )

```

```{r, eval=FALSE, echo=FALSE}

modelsummary(list("MLE"= navco_logit, "Bayesian" = navco_blogit),
             statistic = c("conf.int"),
             gof_map = c('nobs','logLik'),
             coef_rename =coefs,
             output = 'images/logit_table.png'
             )

```

## Bayesian vs. MLE logit in R

![](images/logit_table.png)


## Bayesian vs. MLE logit in R

Since Bayesian models calculate full posterior distributions for coefficients, rather than just a single point that maximizes the likelihood, we can represent uncertainty by sampling from the posterior and looking at how estimates are distributed. 

For instance, here's the distribution of estimates of sampled betas from the model posterior:

```{r, echo=TRUE,eval=FALSE}
pplot<-plot(navco_blogit, "areas", prob = 0.95, prob_outer = 1)

pplot



```

![](images/bayesian_coefs.png)

The shaded areas represent "95% credible intervals", the Bayesian analog to a confidence interval. Their interpretation is simpler: there's a 95% chance that the coefficient falls into this range.


# Interpretation of results

## Interpretation

The way we solve this is ultimately less important than how it changes interpretations. There's a few key differences here when compared to OLS

:::::: columns
:::: {.column width="50%"}
::: incremental
1.  The effect of a one unit increase in X is not constant. It's smaller toward 1 and 0, and steeper around p=.5
2.  Slope coefficients aren't especially meaningful on their own, and aren't comparable across models
3.  Everything impacts everything else. Unobserved heterogeneity impacts the estimate of $\beta$
4.  Metrics like $R^2$ no longer really work (although other measures of model fit are available)
:::
::::

::: {.column width="50%"}
```{r, echo=FALSE}

set.seed(100)
N<-1000
X<-rnorm(N, 0, 1)
ystar<-X * 2 
# the probit link
y_probit<-(ystar + rnorm(N))>0
# the logit link 
y_logit <-(ystar + rlogis(N))>0
probit<-glm(y_probit ~ X, family=binomial(link = probit))
logit<-glm(y_logit ~ X, family=binomial(link = logit))

df<-bind_rows(tibble('Pr(Y=1)' = sort(unique(predict(logit, type='response'))), 
                     'X' =sort(X),
                     'link' = 'logit'
                     ),
              tibble('Pr(Y=1)' = sort(unique(predict(probit, type='response'))), 
                     'X' =sort(X),
                    'link' = 'probit')
              )
               
ggplot(data=df, aes(x=X, y=`Pr(Y=1)`, color=link))  + geom_line(lwd=1.4) + 
  theme_bw()


```
:::
::::::

## Side note: uncontrolled heterogeneity matters here

```{r, echo=T}
set.seed(100)
N<-1000
X<-rnorm(N, 0, 1)
Z<- rnorm(N, 0 ,1)
ystar<-X * 4  + Z * 2
y_logit <- (ystar + rlogis(N))>0
model1<-glm(y_logit ~ X, family=binomial(link = logit))
model2<-glm(y_logit ~ X + Z, family=binomial(link = logit))
```

```{r, echo=FALSE}


modelsummary(list("model 1" =model1, 
                  'model 2' = model2
                  ), 
              coef_rename=TRUE,
             coef_omit = "Intercept",
              estimate  = "{estimate}",  
             statistic = c("conf.int"),
             conf_level = .95,        
 note = "95% CI in brackets",
 gof_omit = 'F|RMSE|R2$|AIC|Log.Lik.',
 output ='kableExtra'
             )

```

## Applying the model


```{r}
#| output-location: slide

navco_logit<-glm(SUCCESS ~  goal_type, 
                 data=navco, 
                 family=binomial(link='logit'))

navco_probit<-glm(SUCCESS ~  goal_type, 
                  data=navco,
                  family=binomial(link='probit'))

modelsummary(list(
  "navco LPM" =  navco_lpm,
  "navco logit" =navco_logit,
  "navco probit"= navco_probit
                  
                  ), 
              coef_rename=coefs,
             coef_omit = "Intercept",
              estimate  = "{estimate}",  
             statistic = c("conf.int"),
             conf_level = .95,        
 note = "95% CI in brackets",
 gof_map = c('nobs', "logLik",'aic', 'bic'),
 output ='kableExtra'
             )


```

## Interpretation: logit intercept

For logit, the coefficients are $ln \frac{p}{1-p}$ of Y=1. So exponentiation can give you some sense for the probabilities if you're used to thinking in those terms. After exponentiation, the intercept is the log odds when everything = 0.

```{r, echo=T}
# exponentiated coefficent
exp(coef(navco_logit))

#  Pr(Y=1) at viol = 0 
pr<-mean(navco$SUCCESS[which(navco$VIOL==0)])
pr
# expressed as an odds ratio = the exponentiated intercept
orat<-pr/(1-pr)
orat
```

## Interpretation: logit coefficient

And the slope coefficient is the impact of a one unit change in X on the relative odds ratio compared to the baseline.

```{r, echo=T}

# exponentiated coefficent
exp(coef(navco_logit))

#  Pr(Y=1) at viol = 1
pr_viol<-mean(navco$SUCCESS[which(navco$VIOL==1)])

pr_viol
# the odds ratio
orat_viol<-pr_viol/(1-pr_viol)

# the relative odds ratio compared to the baseline
orat_viol/orat


```

(Probit coefficients don't work this way, which is partly why they're less popular)

## Interpretation: predicted probabilities and marginal effects

**Rather than try to interpret logit/probit coefficients in their untransformed states, its almost always better to convert them to predicted probabilities and marginal effects at some meaningful levels of the coefficients.**

. . .

-   Predicted probability is just the model prediction for Y=1 at some level of the independent variables

-   Marginal effect is just the predicted effect of a a change in X on the predicted probability of Y

    -   In a linear model the marginal effect is the same thing as the coefficient, but in logit/probit, the marginal effect is different in different places. So we will have to talk about marginal effects *at* some specific point along the curve.

::: notes
Odds ratios are for casinos.
:::

### Predicted probability: logit

In a logit model, the predicted probability for some observation is just the inverse logistic of the predicted linear value of y. Or

$$
P = \frac{1}{1+e^-(B_0+B_1X_1+B_2X_2 +...)}
$$

(less complicated than it looks.)

In plain English, you just take the same plug-in-numbers approach you take to get a prediction from OLS, and then do this:

```{r, echo=T}

# the inverse logit 
inverse_logit<-function(x){ 
  exp(x)/(1+exp(x))
  }

coef(navco_logit)

# success probabilty for non-violent movements
inverse_logit(0.2947995)
mean(navco$SUCCESS[which(navco$VIOL==0)])

```

### Marginal effects: logit

To get the marginal effect from a given point, you just need to see what happens with a one unit change in X for some unit. So the marginal effect of moving from a non-violent campaign to a violent campaign is :

```{r, echo=T}

coef(navco_logit)

# success probability for violent movement - success for non-violent
inverse_logit(0.2947995 + -1.1522498 ) - inverse_logit(0.2947995)


```

### Using Predict

Fortunately, we can just get the predictions using the predict function with a slight modification (and this works for probit models as well)

```{r, echo=T}

table(predict(navco_logit, type='response'))
table(predict(navco_probit, type='response')) # virtually identical!

```

### Using Predict

To get a marginal effect for some other hypothetical value, we can use the newdata argument and create a new data set with specific values assigned to each predictor

```{r, echo=T}

navco_mod2<-glm(SUCCESS ~  VIOL +  SECESSION + percent_pop, data=navco, family='binomial')

# pr success for a non-violent secessionist movement that has 1% popular participation
case_0 <- predict(navco_mod2, type='response', newdata=data.frame("VIOL" = 0,
                                                        "SECESSION" = 1,
                                                         "percent_pop" = .01
                                                        ))
# pr success for a violent secessionist movement that has 2% participation
case_1 <- predict(navco_mod2, type='response', newdata=data.frame("VIOL" = 0,
                                                        "SECESSION" = 1,
                                                        "percent_pop" =.02
                                                        ))


case_1 - case_0

```

### Using Predict

Note that the marginal effect of a one unit increase in a variable is higher or lower depending on our location along the probability curve: as we approach probabilities of zero and one, the slope gets more shallow.

```{r, echo=FALSE}


df<-tibble("VIOL" = 0,
       "SECESSION" = 1, 
       "percent_pop" = seq(-1, 1, by=.01)
       )
df$predict <- predict(navco_mod2, type='response', newdata=df)
df$mfx <- df$predict - lag(df$predict)
# sequence of effects from 0 to 1
ggplot(df, aes(x=predict, y=mfx)) + 
  geom_line() +
  theme_bw() +
  xlab("Pr(success)") +
  ylab("Marginal Effect of percent pop +1")



```

### Average marginal effects

A good way to give a general summary of an effect is the "observed values" or "counterfactual" approach. This entails leaving everything at observed values, manipulating a single "focal" term, and then averaging over the predictions:

```{r}

navco_mod2<-glm(SUCCESS ~  VIOL +  SECESSION + percent_pop, data=navco, family='binomial')

model_data<-navco_mod2$data

# pr success at observed participation
case_0 <- predict(navco_mod2, type='response', newdata=model_data)
# pr success at observed participation +
case_1 <- predict(navco_mod2, type='response', newdata=model_data|>mutate(percent_pop = percent_pop + .01))


mean(case_1 - case_0)


```
. . .

This eliminates the issue of having to choose specific values for the non-focal variables, and amounts to answering: "what would happen if participation increased by 1% for every movement?"

### Marginal effects and standard errors



Getting a standard error for these transformed probabilities can be kind of a pain.

. . .

::::: {.columns}
::: {.column width="50%"}

- We could bootstrap! 

- Or we could use simulation!

- Or, in a Bayesian model, we could just sample from the posterior and transform.

:::
::: {.column width="50%"}

::: fragment

Most statistical software will default to using the faster [delta method](https://bookdown.org/ts_robinson1994/10EconometricTheorems/dm.html) to approximate standard errors.

![Designing Women star Delta Burke, who (I assume) invented the Delta method](https://grantland.com/wp-content/uploads/2015/05/delta-burke-tv-guide.jpg?w=1200)

:::

:::
:::::



### Packages for GLM interpretation

The [`ggeffects`](https://strengejacke.github.io/ggeffects/) package automates a lot of the process here:

### Predictions

We can get adjusted predictions and confidence intervals with one function:

```{r,echo=T}
library(ggeffects)


preds<-predict_response(navco_mod2, 
                        terms="percent_pop[all]",  # prediction at all values of percent_pop
                 condition =c("SECESSION" = 1, "VIOL"=1))  # with these values at 1 and 1

preds


```

## Predictions

And automatically generate a nice plot for the predicted results:

```{r}

plot(preds, 
     show_data = TRUE, jitter=.03)  # plot some relevant data points

```

### Marginal Effects

And once we have a series of predicted probabilities, we can easily estimate average marginal effects across those cases:

```{r, echo=T}


mfx<-preds|>
  test_predictions()

mfx


```

### Marginal Effects

Alternatively, we can generate marginal effects using the `marginaleffects` package. By default, the `avg_comparisons` function will return the average marginal effect of a one unit change in each covariate with all other covariates held at observed values:

```{r}
library(marginaleffects)
avg_comparisons(navco_mod2)

```

### Marginal effects plots

```{r, echo=T}

avg_comparisons(navco_mod2)|>
  modelplot()


```

### Logit/Probit Model comparison

As long as the link function is the same, you can make valid model comparisons using the AIC/BIC values or the log-likelihoods. There are also some fit statistics that are meant to approximate $R^2$ values, but there's no consensus on what they are supposed to mean.

```{r, echo=FALSE}


modelsummary(list("model 1" =navco_logit,
                  "model 2" = navco_mod2
                  ), 
              coef_rename=TRUE,
             coef_omit = "Intercept",
              estimate  = "{estimate}",  
             statistic = c("conf.int"),
             conf_level = .95,        
 note = "95% CI in brackets",
 gof_omit = 'F|RMSE|R2$|AIC|Log.Lik.',
 output ='kableExtra'
             )
```

### Prediction on held-out data

Of course, if you've got the data/time I think the best approach is to compare models based on something empirical like their ability to predict held-out data:

```{r, echo=T}

library(caret)
#  leave one out cross-validation
train.control <- trainControl(method = "LOOCV")
# convert outcomes to factor variables: 
navco<-navco|>
  mutate(outcome = factor(SUCCESS, labels=c("failure", "success")))

cv_0 <- train(outcome ~  VIOL , data=navco, 
              method = "glm",
              family ='binomial',
              trControl = train.control)
cv_1 <- train(outcome ~  VIOL +  SECESSION + participation_ntile, data=navco, 
              method = "glm",
              family ='binomial',
              trControl = train.control)
cv_2 <- train(outcome ~  VIOL , data=navco, 
              method = "rpart",
              trControl = train.control)



```

### Prediction on held-out data

```{r}
cv_0$results
cv_1$results
cv_2$results[1,]


```
